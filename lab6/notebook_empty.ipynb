{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Selekcja cech i redukcja wymiarowości"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "**Uwaga:** upewnij się, że masz scikit-learn w wersji 1.2 lub nowszej. Dzięki temu będzie można ustawić opcję `transform_output=\"pandas\"`, dzięki której wszystkie transformatory w scikit-learn będą nie tylko przyjmować, ale też zwracać DataFrame, zachowując nazwy zmiennych. Będzie to bardzo przydatne w analizie danych."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "\n",
    "\n",
    "sklearn.set_config(transform_output=\"pandas\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analiza cech"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ponownie wykorzystamy zbiór danych [Ames housing](https://www.openintro.org/book/statdata/?data=ames), w którym zadaniem jest przewidywanie wartości domu na podstawie cech budynku, działki, lokalizacji itp. Ma on duży zbiór wejściowych cech, odpowiadający typowym wartościom ze stron handlujących nieruchomościami, więc odpowiada realnym potrzebom analizy biznesowej.\n",
    "\n",
    "Interpretowalność jest tutaj kluczowa, bo każdy sprzedający chce zyskać jak najwięcej. Informacja, które cechy nie mają znaczenia dla kupujących, a na które jest zwracana największa uwaga, mogą np. pokierować decyzjami, które części domu wyremontować przed ogłoszeniem sprzedaży. Z kolei cechy o niskiej ważności warto odrzucić nie tylko, aby pomóc modelowi ML, ale też żeby np. nie wspominać o tych elementach w ogłoszeniu, bo i tak mało kogo obchodzą.\n",
    "\n",
    "Zmniejszenie liczby cech jest też korzystne dla samych firm IT oferujących serwisy sprzedażowe, bo może być ważne dla user experience (UX). Formularz na 70 elementów o różnych cechach domu jest mocno problematyczny do wypełnienia, a taki na 20 jest już znacznie przyjaźniejszy. Takie rzeczy jak najbardziej mogą decydować o tym, którą platformę do ogłoszeń wybiorą sprzedający.\n",
    "\n",
    "Poniższy kod wczytuje dane i dokonuje podziału na zbiór treningowy i testowy.\n",
    "\n",
    "**Uwaga:** będziemy operować na wielu kopiach danych do różnych wariantów procesowania danych. Nadpisanie zmiennej `X_train` z komórki poniżej może skutkować trudnymi do znalezienia bugami, zwróć na to uwagę."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "df = pd.read_csv(\"ames_data.csv\")\n",
    "\n",
    "# remove dots from names to match data_description.txt\n",
    "df.columns = [col.replace(\".\", \"\") for col in df.columns]\n",
    "\n",
    "df = df.drop([\"Order\", \"PID\"], axis=\"columns\")\n",
    "df = df.loc[~df[\"Neighborhood\"].isin([\"GrnHill\", \"Landmrk\"]), :]\n",
    "\n",
    "df = df.loc[df[\"GrLivArea\"] <= 4000, :]\n",
    "\n",
    "df.loc[:, \"SalePrice\"] = np.log1p(df.loc[:, \"SalePrice\"])\n",
    "\n",
    "def replace_na(df: pd.DataFrame, col: str, value) -> None:\n",
    "    df.loc[:, col] = df.loc[:, col].fillna(value)\n",
    "\n",
    "# Alley : data description says NA means \"no alley access\"\n",
    "replace_na(df, \"Alley\", value=\"None\")\n",
    "\n",
    "# BedroomAbvGr : NA most likely means 0\n",
    "replace_na(df, \"BedroomAbvGr\", value=0)\n",
    "\n",
    "# BsmtQual etc : data description says NA for basement features is \"no basement\"\n",
    "replace_na(df, \"BsmtQual\", value=\"No\")\n",
    "replace_na(df, \"BsmtCond\", value=\"No\")\n",
    "replace_na(df, \"BsmtExposure\", value=\"No\")\n",
    "replace_na(df, \"BsmtFinType1\", value=\"No\")\n",
    "replace_na(df, \"BsmtFinType2\", value=\"No\")\n",
    "replace_na(df, \"BsmtFullBath\", value=0)\n",
    "replace_na(df, \"BsmtHalfBath\", value=0)\n",
    "replace_na(df, \"BsmtUnfSF\", value=0)\n",
    "\n",
    "# Condition : NA most likely means Normal\n",
    "replace_na(df, \"Condition1\", value=\"Norm\")\n",
    "replace_na(df, \"Condition2\", value=\"Norm\")\n",
    "\n",
    "# Electrical : NA most likely means standard\n",
    "replace_na(df, \"Electrical\", value=\"SBrkr\")\n",
    "\n",
    "# External stuff : NA most likely means average\n",
    "replace_na(df, \"ExterCond\", value=\"TA\")\n",
    "replace_na(df, \"ExterQual\", value=\"TA\")\n",
    "\n",
    "# Fence : data description says NA means \"no fence\"\n",
    "replace_na(df, \"Fence\", value=\"No\")\n",
    "\n",
    "# Functional : data description says NA means typical\n",
    "replace_na(df, \"Functional\", value=\"Typ\")\n",
    "\n",
    "# GarageType etc : data description says NA for garage features is \"no garage\"\n",
    "replace_na(df, \"GarageType\", value=\"No\")\n",
    "replace_na(df, \"GarageFinish\", value=\"No\")\n",
    "replace_na(df, \"GarageQual\", value=\"No\")\n",
    "replace_na(df, \"GarageCond\", value=\"No\")\n",
    "replace_na(df, \"GarageArea\", value=0)\n",
    "replace_na(df, \"GarageCars\", value=0)\n",
    "\n",
    "# HalfBath : NA most likely means no half baths above grade\n",
    "replace_na(df, \"HalfBath\", value=0)\n",
    "\n",
    "# HeatingQC : NA most likely means typical\n",
    "replace_na(df, \"HeatingQC\", value=\"Ta\")\n",
    "\n",
    "# KitchenAbvGr : NA most likely means 0\n",
    "replace_na(df, \"KitchenAbvGr\", value=0)\n",
    "\n",
    "# KitchenQual : NA most likely means typical\n",
    "replace_na(df, \"KitchenQual\", value=\"TA\")\n",
    "\n",
    "# LotFrontage : NA most likely means no lot frontage\n",
    "replace_na(df, \"LotFrontage\", value=0)\n",
    "\n",
    "# LotShape : NA most likely means regular\n",
    "replace_na(df, \"LotShape\", value=\"Reg\")\n",
    "\n",
    "# MasVnrType : NA most likely means no veneer\n",
    "replace_na(df, \"MasVnrType\", value=\"None\")\n",
    "replace_na(df, \"MasVnrArea\", value=0)\n",
    "\n",
    "# MiscFeature : data description says NA means \"no misc feature\"\n",
    "replace_na(df, \"MiscFeature\", value=\"No\")\n",
    "replace_na(df, \"MiscVal\", value=0)\n",
    "\n",
    "# OpenPorchSF : NA most likely means no open porch\n",
    "replace_na(df, \"OpenPorchSF\", value=0)\n",
    "\n",
    "# PavedDrive : NA most likely means not paved\n",
    "replace_na(df, \"PavedDrive\", value=\"N\")\n",
    "\n",
    "# PoolQC : data description says NA means \"no pool\"\n",
    "replace_na(df, \"PoolQC\", value=\"No\")\n",
    "replace_na(df, \"PoolArea\", value=0)\n",
    "\n",
    "# SaleCondition : NA most likely means normal sale\n",
    "replace_na(df, \"SaleCondition\", value=\"Normal\")\n",
    "\n",
    "# ScreenPorch : NA most likely means no screen porch\n",
    "replace_na(df, \"ScreenPorch\", value=0)\n",
    "\n",
    "# TotRmsAbvGrd : NA most likely means 0\n",
    "replace_na(df, \"TotRmsAbvGrd\", value=0)\n",
    "\n",
    "# Utilities : NA most likely means all public utilities\n",
    "replace_na(df, \"Utilities\", value=\"AllPub\")\n",
    "\n",
    "# WoodDeckSF : NA most likely means no wood deck\n",
    "replace_na(df, \"WoodDeckSF\", value=0)\n",
    "\n",
    "# CentralAir : NA most likely means No\n",
    "replace_na(df, \"CentralAir\", value=\"N\")\n",
    "\n",
    "# EnclosedPorch : NA most likely means no enclosed porch\n",
    "replace_na(df, \"EnclosedPorch\", value=0)\n",
    "\n",
    "# FireplaceQu : data description says NA means \"no fireplace\"\n",
    "replace_na(df, \"FireplaceQu\", value=\"No\")\n",
    "replace_na(df, \"Fireplaces\", value=0)\n",
    "\n",
    "# SaleCondition : NA most likely means normal sale\n",
    "replace_na(df, \"SaleCondition\", value=\"Normal\")\n",
    "\n",
    "df = df.replace(\n",
    "    {\n",
    "        \"MSSubClass\": {\n",
    "            20: \"SC20\",\n",
    "            30: \"SC30\",\n",
    "            40: \"SC40\",\n",
    "            45: \"SC45\",\n",
    "            50: \"SC50\",\n",
    "            60: \"SC60\",\n",
    "            70: \"SC70\",\n",
    "            75: \"SC75\",\n",
    "            80: \"SC80\",\n",
    "            85: \"SC85\",\n",
    "            90: \"SC90\",\n",
    "            120: \"SC120\",\n",
    "            150: \"SC150\",\n",
    "            160: \"SC160\",\n",
    "            180: \"SC180\",\n",
    "            190: \"SC190\",\n",
    "        },\n",
    "        \"MoSold\": {\n",
    "            1: \"Jan\",\n",
    "            2: \"Feb\",\n",
    "            3: \"Mar\",\n",
    "            4: \"Apr\",\n",
    "            5: \"May\",\n",
    "            6: \"Jun\",\n",
    "            7: \"Jul\",\n",
    "            8: \"Aug\",\n",
    "            9: \"Sep\",\n",
    "            10: \"Oct\",\n",
    "            11: \"Nov\",\n",
    "            12: \"Dec\",\n",
    "        },\n",
    "    }\n",
    ")\n",
    "\n",
    "df = df.replace(\n",
    "    {\n",
    "        \"Alley\": {\"None\": 0, \"Grvl\": 1, \"Pave\": 2},\n",
    "        \"BsmtCond\": {\"No\": 0, \"Po\": 1, \"Fa\": 2, \"TA\": 3, \"Gd\": 4, \"Ex\": 5},\n",
    "        \"BsmtExposure\": {\"No\": 0, \"Mn\": 1, \"Av\": 2, \"Gd\": 3},\n",
    "        \"BsmtFinType1\": {\n",
    "            \"No\": 0,\n",
    "            \"Unf\": 1,\n",
    "            \"LwQ\": 2,\n",
    "            \"Rec\": 3,\n",
    "            \"BLQ\": 4,\n",
    "            \"ALQ\": 5,\n",
    "            \"GLQ\": 6,\n",
    "        },\n",
    "        \"BsmtFinType2\": {\n",
    "            \"No\": 0,\n",
    "            \"Unf\": 1,\n",
    "            \"LwQ\": 2,\n",
    "            \"Rec\": 3,\n",
    "            \"BLQ\": 4,\n",
    "            \"ALQ\": 5,\n",
    "            \"GLQ\": 6,\n",
    "        },\n",
    "        \"BsmtQual\": {\"No\": 0, \"Po\": 1, \"Fa\": 2, \"TA\": 3, \"Gd\": 4, \"Ex\": 5},\n",
    "        \"ExterCond\": {\"Po\": 1, \"Fa\": 2, \"TA\": 3, \"Gd\": 4, \"Ex\": 5},\n",
    "        \"ExterQual\": {\"Po\": 1, \"Fa\": 2, \"TA\": 3, \"Gd\": 4, \"Ex\": 5},\n",
    "        \"FireplaceQu\": {\"No\": 0, \"Po\": 1, \"Fa\": 2, \"TA\": 3, \"Gd\": 4, \"Ex\": 5},\n",
    "        \"Functional\": {\n",
    "            \"Sal\": 1,\n",
    "            \"Sev\": 2,\n",
    "            \"Maj2\": 3,\n",
    "            \"Maj1\": 4,\n",
    "            \"Mod\": 5,\n",
    "            \"Min2\": 6,\n",
    "            \"Min1\": 7,\n",
    "            \"Typ\": 8,\n",
    "        },\n",
    "        \"GarageCond\": {\"No\": 0, \"Po\": 1, \"Fa\": 2, \"TA\": 3, \"Gd\": 4, \"Ex\": 5},\n",
    "        \"GarageQual\": {\"No\": 0, \"Po\": 1, \"Fa\": 2, \"TA\": 3, \"Gd\": 4, \"Ex\": 5},\n",
    "        \"HeatingQC\": {\"Po\": 1, \"Fa\": 2, \"TA\": 3, \"Gd\": 4, \"Ex\": 5},\n",
    "        \"KitchenQual\": {\"Po\": 1, \"Fa\": 2, \"TA\": 3, \"Gd\": 4, \"Ex\": 5},\n",
    "        \"LandSlope\": {\"Sev\": 1, \"Mod\": 2, \"Gtl\": 3},\n",
    "        \"LotShape\": {\"IR3\": 1, \"IR2\": 2, \"IR1\": 3, \"Reg\": 4},\n",
    "        \"PavedDrive\": {\"N\": 0, \"P\": 1, \"Y\": 2},\n",
    "        \"PoolQC\": {\"No\": 0, \"Fa\": 1, \"TA\": 2, \"Gd\": 3, \"Ex\": 4},\n",
    "        \"Street\": {\"Grvl\": 1, \"Pave\": 2},\n",
    "        \"Utilities\": {\"ELO\": 1, \"NoSeWa\": 2, \"NoSewr\": 3, \"AllPub\": 4},\n",
    "    }\n",
    ")\n",
    "\n",
    "y = df.pop(\"SalePrice\")\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    df, y, test_size=0.3, random_state=0\n",
    ")\n",
    "\n",
    "X_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analiza cech kategorycznych i numerycznych wykorzystuje często różne metody i wymaga innego preprocessingu. Dodatkowo one-hot encoding bardzo utrudnia interpretację cech kategorycznych, dlatego trzeba się zastanowić, na którym etapie przetwarzania danych dokonujemy sprawdzania ważności cech danym algorytmem.\n",
    "\n",
    "Na początek skupimy się na zmiennych numerycznych. Podstawową metodą jest sprawdzenie wariancji tych danych. Wymaga to danych bez wartości brakujących i o tej samej skali.\n",
    "\n",
    "**Uwaga:** nie można tutaj dokonać standaryzacji! Wtedy dzieli się przez odchylenie standardowe, więc wszystko ma wariancję 1. Trzeba dokonać przeskalowania przez min-max scaling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "\n",
    "X_train_num = X_train.select_dtypes(exclude=\"object\")\n",
    "\n",
    "numerical_pipeline = make_pipeline(\n",
    "    SimpleImputer(strategy=\"median\"), \n",
    "    MinMaxScaler()\n",
    ")\n",
    "\n",
    "X_train_num = numerical_pipeline.fit_transform(X_train_num)\n",
    "X_train_num"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do usuwania cech o niskiej wariancji służy klasa `VarianceThreshold`. Jaki jednak ustawić próg wariancji? To dobrze byłoby najpierw sprawdzić na wykresie, bo w zależności od zbioru dane po prostu ogólnie mogą mieć mniejszą lub większą zmienność.\n",
    "\n",
    "**Zadanie 1 (0.5 punktu)**\n",
    "\n",
    "1. Oblicz wariancję dla cech w danych treningowych.\n",
    "2. Posortuj cechy w kolejności od największej do najmniejszej wariancji.\n",
    "3. Wypisz cechy o najmniejszej wariancji (razem z jej wartościami).\n",
    "4. Przedstaw to na wykresie słupkowym (bar plot). Pamiętaj o tytule wykresu, opisaniu osi i cech. Upewnij się, że ma odpowiednią wielkość, aby nazwy cech były czytelne.\n",
    "5. Zinterpretuj wykres i skomentuj:\n",
    "   - jaki twoim zdaniem byłby sensowny próg wariancji?\n",
    "   - czy jest cecha / cechy, które twoim zdaniem są tak mało zmienne, że są bezużyteczne?\n",
    "   - czy twoim zdaniem cechy o największej wariancji wydają się bardzo użyteczne?\n",
    "\n",
    "Podpowiedź: każdy z podpunktów da się zrobić jedną metodą na DataFrame'ie."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "// skomentuj tutaj"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wariancja sprawdza tylko zmienność cech, a w szczególności nie uwzględnia żadnych interakcji między cechami. Sprawdźmy zatem liniowe korelacje pomiędzy cechami.\n",
    "\n",
    "Jako że naszych cech jest bardzo dużo, to najpier warto narysować heatmapę korelacji, bez żadnych liczb, i wizualnie oszacować, czy mamy mocno skorelowane cechy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "\n",
    "corr_matrix = X_train_num.corr()\n",
    "# remove upper triangle, to plot only lower triangle of correlations\n",
    "corr_matrix = corr_matrix.where(~np.triu(np.ones(corr_matrix.shape)).astype(bool))\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.heatmap(corr_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Na oko nie ma tutaj wielu bardzo mocnych korelacji, rzędu 0.8 albo więcej. Mamy jednak dużo cech, więc warto popatrzeć też na same pary korelacji między kolumnami.\n",
    "\n",
    "Tutaj liczy się ogólna siła korelacji, więc posortujemy malejąco po wartości bezwzględnej korelacji."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    corr_matrix\n",
    "    .stack()\n",
    "    .sort_values(ascending=False, key=lambda x: abs(x))\n",
    "    .head(10)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jak widać, mamy jedną mocno skorelowaną parę - jakość i stan garażu. Ma to sens, w końcu to praktycznie zduplikowana informacja. Mamy też kilka par cech z dość mocnymi korelacjami powyżej 0.8. Ale biorąc pod uwagę, jak dużo cech mamy, to jest generalnie nieźle.\n",
    "\n",
    "Ostatnią prostą miarą, którą zweryfikujemy, jest korelacja (konkretnie Pearson r correlation) między cechą a zmienną zależną. Scikit-learn implementuje ją jako funkcję, która przyjmuje macierz X, wektor y i zwraca wektor z wartością korelacji dla każdej cechy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import r_regression\n",
    "\n",
    "\n",
    "importances_corr_r = r_regression(X_train_num, y_train)\n",
    "\n",
    "# change to pd.Series for easier sorting and plotting\n",
    "importances_corr_r = pd.Series(importances_corr_r, index=X_train_num.columns)\n",
    "importances_corr_r = importances_corr_r.sort_values(ascending=False)\n",
    "\n",
    "# create bar plot\n",
    "importances_corr_r.plot.bar(figsize=(10, 5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Zadanie 2 (1 punkt)**\n",
    "\n",
    "1. Czy uzyskane korelacje dla cech są zgodne z oczekiwaniami i ogólnie mają sens?\n",
    "2. Mamy sporo cech o dość mocnej korelacji (0.5 lub więcej) - czy dobrze, czy źle, i dlaczego?\n",
    "3. Mamy cechę (LotShape) o wyraźniej negatywnej korelacji. Czy takie cechy są użyteczne?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "// skomentuj tutaj"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Przydałoby się w końcu przestać ignorować cechy kategoryczne i sprawdzić ważność faktycznie wszystkich cech. Można do tego użyć mutual information. Scikit-learn implementuje je jako funkcję, która przyjmuje X, y oraz informację o tym, które cechy są dyskretne. Mogą to być i cechy kategoryczne, i numeryczne, które mają wartości całkowite.\n",
    "\n",
    "Mamy jednak problem - na wejściu mamy zmienne dyskretne, które zamieniliśmy na ciągłe przez min-max scaling. W przypadku mutual information nie należy tego robić, bo obsługuje wprost zmienne dyskretne.\n",
    "\n",
    "**Zadanie 3 (3 punkty)**\n",
    "\n",
    "1. Znajdź typy kolumn w danych:\n",
    "   - kategoryczne: wszystkie typu \"object\"\n",
    "   - numeryczne: wszystkie poza typem \"object\"\n",
    "2. Stwórz pipeline do przetwarzania danych:\n",
    "   - skorzystaj z klasy `ColumnTransformer`, przetwarzając w odpowiedni sposób zmienne (jak na laboratorium 1)\n",
    "   - `OrdinalEncoder` do zmiennych kategorycznych, żeby zakodować je jako liczby całkowite\n",
    "   - `SimpleImputer` do zmiennych numerycznych, aby imputować wartości brakujące medianą\n",
    "   - użyj opcji `verbose_feature_names_out=False`, aby nazwy zmiennych nie były modyfikowane\n",
    "3. Przetransformuj macierz `X_train`, tworząc `X_train_mi`.\n",
    "4. Popraw z powrotem typy z pomocą metody `.convert_dtypes()` ([dokumentacja](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.convert_dtypes.html)) - zapewni to, że zmienne całkowitoliczbowe będą miały odpowiedni typ.\n",
    "5. Oblicz wartości mutual information dla zmiennych. Pamiętaj o podaniu, które zmienne są dyskretne, oraz o przekazaniu `random_state=0`.\n",
    "6. Przedstaw ważności cech według MI na wykresie, posortowane malejąco.\n",
    "7. Dokonaj interpretacji cech:\n",
    "   - Czy nienadzorowane miary, których użyliśmy wcześniej (wariancja i korelacja) zgadzają się ważnością cech z MI?\n",
    "   - Czy MI i korelacja ze zmienną zależną zgadzają się ze sobą? Jeżeli nie, to której mierze można by zaufać bardziej?\n",
    "   - Czy mamy tutaj cechy, które wyraźnie warto usunąć wedle miary MI? Czy ma to sens, patrząc na znaczenie tych cech?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.feature_selection import mutual_info_regression\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "\n",
    "\n",
    "categorical_features = ...\n",
    "numerical_features = ...\n",
    "\n",
    "column_transformer = ...\n",
    "\n",
    "X_mi = ...\n",
    "\n",
    "# correct data types\n",
    "X_mi = ...\n",
    "\n",
    "# get boolean mask with discrete columns\n",
    "discrete_features = ...\n",
    "\n",
    "importances_mi = ...\n",
    "\n",
    "# plot\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "// skomentuj tutaj"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Na obecnym etapie widać, że co najmniej kilka cech można by wyeliminować z naszego zbioru, a pewien zbiór wyraźnie dominuje. Używaliśmy jednak tylko metod typu filter. Wypróbujmy jeszcze metody typu embedded i wrapper.\n",
    "\n",
    "Najpopularniejsze metody embedded to regresja liniowa i las losowy. Zgodnie z wynikami z laboratorium 1 użyjemy ridge regression (z regularyzacją L2), żeby zminimalizować przeuczenie. Co ważne, trzeba sprawdzić, czy te metody uzyskują dobre wyniki i czy nie przeuczają - metody embedded i wrapper wymagają dobrych modeli, aby dobrze oceniać ważność cech.\n",
    "\n",
    "Pewną wadą tych metod w kontekście tego zbioru jest to, że nie działają dla zmiennych kategorycznych i musimy dokonać one-hot encodingu. Otrzymamy więcej zmiennych i trochę inne informacje o ważności cech - może się na przykład okazać, że tylko 1 czy 2 wartości dla zmiennej kategorycznej są mało ważne. To także przydatna informacja, bo można wtedy na przykład gromadzić mniej kategorii w przyszłości."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "\n",
    "categorical_features = X_train.select_dtypes(include=\"object\").columns\n",
    "numerical_features = X_train.select_dtypes(exclude=\"object\").columns\n",
    "\n",
    "categorical_pipeline = OneHotEncoder(\n",
    "    drop=\"first\", sparse_output=False, handle_unknown=\"ignore\"\n",
    ")\n",
    "\n",
    "numerical_pipeline = make_pipeline(SimpleImputer(strategy=\"median\"), MinMaxScaler())\n",
    "\n",
    "column_transformer = ColumnTransformer(\n",
    "    [\n",
    "        (\"cat_pipeline\", categorical_pipeline, categorical_features),\n",
    "        (\"num_pipeline\", numerical_pipeline, numerical_features),\n",
    "    ],\n",
    "    verbose_feature_names_out=False\n",
    ")\n",
    "column_transformer.fit(X_train)\n",
    "\n",
    "X_train_processed = column_transformer.transform(X_train)\n",
    "X_test_processed = column_transformer.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "\n",
    "\n",
    "def assess_regression_model(model, X_train, X_test, y_train, y_test) -> None:\n",
    "    y_pred_train = model.predict(X_train)\n",
    "    y_pred_test = model.predict(X_test)\n",
    "\n",
    "    y_train_dollars = np.expm1(y_train)\n",
    "    y_test_dollars = np.expm1(y_test)\n",
    "\n",
    "    y_pred_train_dollars = np.expm1(y_pred_train)\n",
    "    y_pred_test_dollars = np.expm1(y_pred_test)\n",
    "\n",
    "    rmse_train_dollars = mean_squared_error(\n",
    "        y_train_dollars, y_pred_train_dollars, squared=False\n",
    "    )\n",
    "    rmse_test_dollars = mean_squared_error(\n",
    "        y_test_dollars, y_pred_test_dollars, squared=False\n",
    "    )\n",
    "\n",
    "    print(f\"Train RMSE: {rmse_train_dollars:.2f}$\")\n",
    "    print(f\"Test RMSE: {rmse_test_dollars:.2f}$\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.linear_model import LinearRegression, Ridge\n",
    "\n",
    "\n",
    "reg_linear = LinearRegression(fit_intercept=False)\n",
    "reg_linear.fit(X_train_processed, y_train)\n",
    "\n",
    "reg_ridge = Ridge(random_state=0)\n",
    "reg_ridge.fit(X_train_processed, y_train)\n",
    "\n",
    "reg_rf = RandomForestRegressor(random_state=0)\n",
    "reg_rf.fit(X_train_processed, y_train)\n",
    "\n",
    "print(\"Linear regression\")\n",
    "assess_regression_model(reg_linear, X_train_processed, X_test_processed, y_train, y_test)\n",
    "print()\n",
    "print(\"Ridge regression\")\n",
    "assess_regression_model(reg_ridge, X_train_processed, X_test_processed, y_train, y_test)\n",
    "print()\n",
    "print(\"Random Forest regression\")\n",
    "assess_regression_model(reg_rf, X_train_processed, X_test_processed, y_train, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Co ciekawe, w przypadku tego zbioru modele liniowe dają lepsze wyniki od złożonego lasu losowego! Może być to kwestia rozmiaru danych, lub po prostu potrzeby tuningu hiperparametrów. Trzeba jednak zauważyć, że każdy z modeli przeucza, nawet ridge regression, a selekcja cech redukuje overfitting dzięki usuwaniu szumu w danych.\n",
    "\n",
    "W takim wypadku ewidentnie lepiej jest oprzeć się na ważności cech z modeli liniowych. Są to po prostu wagi z atrybutu `.coef_`. Jako że cech po one-hot encodingu jest bardzo dużo, to narysujemy wykres tylko dla najlepszych cech.\n",
    "\n",
    "Cechy o wadze poniżej 0.01 można uznać za naprawdę słabe, bo mają minimalną wagę. Sprawdzimy, ile ich jest, oraz jakie są najmniej ważne według regresji liniowej cechy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importances_ridge = reg_ridge.coef_\n",
    "\n",
    "# change to pd.Series for easier sorting and plotting\n",
    "importances_ridge = pd.Series(importances_ridge, index=X_train_processed.columns)\n",
    "importances_ridge = importances_ridge.sort_values(ascending=False, key=lambda x: abs(x))\n",
    "\n",
    "# create horizontal bar plot for best features\n",
    "importances_ridge.head(10).plot.barh(\n",
    "    title=\"Highest feature importances - ridge regression\",\n",
    "    figsize=(12, 5)\n",
    ")\n",
    "plt.show()\n",
    "\n",
    "# print number of very low weight features\n",
    "num_low_weight_features = (importances_ridge.abs() <= 0.01).sum()\n",
    "print(\"Number of features with weight <= 0.01:\", num_low_weight_features)\n",
    "print(f\"This is {100 * num_low_weight_features / len(importances_ridge):.2f}% of all features\")\n",
    "\n",
    "print(\"Features with lowest weights:\")\n",
    "importances_ridge.tail(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prawie 1/4 naszych cech ma praktycznie zerowy wpływ na predykcje regresji liniowej! Oczywiście w sumie ten wpływ może być całkiem spory, ale warto zauważyć, że jeżeli chcielibyśmy ostro redukować ilość zbieranych cech, np. ze względu na koszty przechowywania albo wymagania prawne, to jesteśmy w stanie to zrobić i prawdopodobnie nie obniży to bardzo wyników naszego modelu. Pamiętajmy też, że regresja liniowa skaluje się gorzej z liczbą cech, niż z liczbą próbek, więc tutaj różnica w szybkości może być faktycznie zauważalna.\n",
    "\n",
    "Na koniec zostały nam metody typu wrapper. Do analizy cech szczególnie interpretowalne jest permutation feature importance, dlatego go teraz użyjemy. Ze względu na to, że nasz zbiór jest mały, użyjemy wersji z walidacją skrośną.\n",
    "\n",
    "**Zadanie 4 (2 punkty)**\n",
    "\n",
    "1. Stwórz obiekt `KFold` ([dokumentacja](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.KFold.html)), dzieląc zbiór treningowy na 5 foldów do walidacji skrośnej.\n",
    "2. W pętli dla kolejnych foldów:\n",
    "   1. Wytnij ze zbioru treningowego podzbiór treningowy i walidacyjny, odpowiadający danemu foldowi (patrz przykład w dokumentacji).\n",
    "   2. Wytrenuj ridge regression na stworzonym podzbiorze treningowym.\n",
    "   3. Oblicz permutation feature importance ([dokumentacja](https://scikit-learn.org/stable/modules/generated/sklearn.inspection.permutation_importance.html#sklearn.inspection.permutation_importance)) na zbiorze walidacyjnym:\n",
    "      - jako metryki (`scoring`) użyj stworzonego poniżej `rmse`\n",
    "      - użyj 100 powtórzeń\n",
    "      - pamiętaj o `random_state=0`\n",
    "      - może się przydać `n_jobs=-1`\n",
    "      - wyciągnij średnie ważności (`importances_mean`) z wynikowej struktury\n",
    "   4. Dopisz ważność do listy (tak, żeby każdy fold dał 1 wynik walidacyjny).\n",
    "3. Oblicz średnią ważność z foldów (mogą się przydać `np.vstack()` i `np.mean()` z odpowiednią osią).\n",
    "4. Narysuj wykres słupkowy 10 najważniejszych cech według permutation importance.\n",
    "5. Sprawdź, czy są cechy, które nie wpłynęły na wynik, tj. praktycznie nie różnią się od zera (przyda się `np.isclose()`). Jeżeli tak, to wypisz, ile ich jest i jaki procent liczby cech stanowią.\n",
    "6. Skomentuj:\n",
    "   - czy cechy pokrywają się z tym, co widzieliśmy do tej pory?\n",
    "   - czy było warto obliczać taki feature importance, czy wnioski z samej metody embedded były wystarczające?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import make_scorer\n",
    "\n",
    "\n",
    "rmse = make_scorer(mean_squared_error, greater_is_better=False, squared=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create KFold\n",
    "kfold = ...\n",
    "\n",
    "# accumulate validation permutation importance scores\n",
    "scores = []\n",
    "\n",
    "\n",
    "# iterate through fold indexes\n",
    "for train_index, valid_index in kfold.split(X_train_processed):\n",
    "    # create X_train_fold, X_valid_fold, y_train_fold and y_valid_fold using indexes\n",
    "    \n",
    "    # train ridge regression\n",
    "    \n",
    "    # calculate permutation importance\n",
    "\n",
    "    # extract mean and append to scores\n",
    "\n",
    "\n",
    "# stack and calculate mean importances\n",
    "\n",
    "# plot\n",
    "\n",
    "# check unimportant features\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "// skomentuj tutaj"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dokonaliśmy już wielu analiz i wiemy sporo o naszych cechach. Na cele analityczne tak naprawdę tyle by wystarczyło - możemy skupić się na niektórych, inne usunąć. Ta część to tak naprawdę business intelligence (BI), gdzie w tym wypadku skorzystaliśmy z uczenia maszynowego do głębszej analizy danych.\n",
    "\n",
    "Teraz czas przejść do zastosowania selekcji cech w samym ML i budowaniu modeli predykcyjnych."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Selekcja cech"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wykorzystamy teraz posiadane informacje, żeby dokonać selekcji cech.\n",
    "\n",
    "Co do surowych, wejściowych cech wiemy, że:\n",
    "- `Utilities` ma bardzo niską wariancję\n",
    "- `GarageCond` i `GarageQual` są bardzo skorelowane\n",
    "- jest szereg cech o praktycznie zerowej wartości wedle mutual information\n",
    "\n",
    "To ostatnie będzie też zachodzić po one-hot encodingu - jeżeli cecha na wejściu jest bezużyteczna, to tym bardziej będzie bezużyteczna, kiedy potniemy ją na więcej cech. Możemy więc tego bezpiecznie dokonać.\n",
    "\n",
    "**Zadanie 5 (2 punkty)**\n",
    "\n",
    "Stwórz pipeline do przetwarzania zmiennych. Do usuwania kolumn użyj stworzonej poniżej klasy `DropFeatures`.\n",
    "\n",
    "Kroki to po kolei:\n",
    "1. `DropFeatures`, które usuwa kolumny `Utilities` i `GarageCond`\n",
    "2. `ColumnTransformer`, który:\n",
    "   - dla cech kategorycznych robi `OneHotEncoder`, pamiętaj o `drop=\"first\", sparse_output=False, handle_unknown=\"ignore\"`\n",
    "   - dla cech numerycznych robi pipeline `SimpleImputer` oraz `MinMaxScaler`\n",
    "   - użyj opcji `verbose_feature_names_out=False`, aby nazwy zmiennych nie były modyfikowane nazwą pipeline'\n",
    "3. Selekcja cech z użyciem `SelectPercentile` ([dokumentacja](https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.SelectPercentile.html)), aby usunąć 10% najsłabszych cech według mutual information. Jako funkcję do obliczania wyniku wykorzystaj mutual information. Uwaga: musisz ręcznie stworzyć `score_func`, żeby podać `random_state=0`, stworzono taki obiekt poniżej.\n",
    "\n",
    "Przetransformowane `X_train` i `X_test` zapisz w zmiennych `X_train_preproc` i `X_test_preproc`.\n",
    "\n",
    "**Uwaga:** przy tworzeniu listy zmiennych numerycznych pamiętaj, żeby usunąć z niej nazwy usuwanych kolumn, tj. `Utilities` i `GarageCond`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DropFeatures:\n",
    "    def __init__(self, features: str | list[str]):\n",
    "        self.features = features\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X, y=None):\n",
    "        return X.drop(columns=self.features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import mutual_info_regression\n",
    "\n",
    "\n",
    "scorer_mi = lambda X, y: mutual_info_regression(X, y, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mamy teraz pełen zestaw cech, gotowy do treningu regresji liniowej."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_ridge = Ridge(random_state=0)\n",
    "reg_ridge.fit(X_train_preproc, y_train)\n",
    "\n",
    "assess_regression_model(reg_ridge, X_train_preproc, X_test_preproc, y_train, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wynik testowy jest bardzo podobny, natomiast błąd treningowy jest większy. Czy to gorzej? Otóż niekoniecznie - w końcu teraz przeuczamy mniej. Taki model może być mniej podatny na szum i błędne pomiary, dając bardziej pewne (robust) wyniki w przyszłości.\n",
    "\n",
    "Nie wykorzystaliśmy jednak jeszcze ważnej metody selekcji cech - Recursive Feature Elimination (RFE).\n",
    "\n",
    "**Zadanie 6 (1 punkt)**\n",
    "\n",
    "1. Zastosuj RFECV ([dokumentacja](https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.RFECV.html)) do selekcji cech:\n",
    "   - użyj ridge regression jako estymatora bazowego\n",
    "   - wybierz co najmniej 80% oryginalnych cech\n",
    "   - zastosuj 10-krotną walidację skrośną\n",
    "   - pamiętaj o `random_state=0`\n",
    "   - może się przydać `n_jobs=-1`\n",
    "2. Przetransformuj `X_train_preproc` i `X_test_preproc`, tworząc `X_train_reduced` i `X_test_reduced`. Sprawdź, ile cech usunięto.\n",
    "3. Wytrenuj ridge regression na zredukowanych cechach.\n",
    "4. Sprawdź wyniki na zbiorze treningowym i testowym (skorzystaj z `assess_regression_model`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sumarycznie wyeliminowaliśmy sporo cech, a nasz model uzyskuje wyniki bardzo podobne do wyjściowych. Być może uda nam się zyskać jeszcze więcej, jeżeli użyjemy redukcji wymiarowości."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Redukcja wymiarowości"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mamy dużo cech rzadkich po one-hot encodingu, ponad 100. Redukcja takich cech to typowe zadanie dla SVD, które liniowo zredukuje te cechy z powrotem do mniejszej liczby cech gęstych. W scikit-learn jest ono zaimplementowane w klasie `TruncatedSVD`, która jako swój główny argument przyjmuje `n_compoments`, czyli liczbę docelowych cech. Dla uproszczenia przyjmiemy 60, czyli około połowy cech rzadkich po one-hot encodingu.\n",
    "\n",
    "**Zadanie 7 (0.5 punktu)**\n",
    "\n",
    "Skopiuj pipeline z zadania 5, dodając `TrucatedSVD` do pipeline'u dla zmiennych kategorycznych, z `n_components=60` i `random_state=0`. Wytrenuj ridge regression na wynikowych cechach i zmierz wynik. Czy udało się go poprawić?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Na koniec wykorzystamy najpopularniejszą metodę redukcji wymiaru, czyli PCA. Jako że nasze cechy przeszły już tyle czyszczenia i redukcji, to nie ma raczej sensu ich dalsza obróbka. Zamiast tego możemy je zrzutować do 2D - jeżeli wizualizacja będzie dobrej jakości, i będzie pokrywać się z naszą intuicją, to znaczy, że mamy dobry zestaw cech.\n",
    "\n",
    "**Uwaga:** poniższy kod zakłada, że cechy po transformacji z ostatniego zadania są w zmiennej `X_train_preproc`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "\n",
    "pca = PCA(n_components=2, random_state=0)\n",
    "pca.fit(X_train_preproc)\n",
    "\n",
    "\n",
    "X_train_embed = pca.transform(X_train_preproc)\n",
    "X_train_embed = X_train_embed.values\n",
    "\n",
    "plt.scatter(X_train_embed[:, 0], X_train_embed[:, 1], c=y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jak widać, nawet po ekstremalnej liniowej redukcji do 2D nasze dane zachowują w miarę strukturę, z większymi (jaśniejszymi) wartościami po prawej, i mniejszymi (ciemniejszymi), w prawym dolnym rogu. Jeżeli liniowa redukcja dobrze działa, to znaczy, że wejściowe cechy były na tyle dobre, że nawet ich liniowa kombinacja zachowuje dużo informacji.\n",
    "\n",
    "Takie wykresy są często nieco bardziej informatywne w przypadku klasyfikacji, gdzie dodatkowo możemy obserwować klastry formowane przez klasy, ale nawet przy tej regresji widać, że efekty są całkiem sensowne."
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Zadanie dodatkowe (3 punkty)\n",
    "\n",
    "Na zbiorze danych [Polish companies bankruptcy](https://archive.ics.uci.edu/ml/datasets/Polish+companies+bankruptcy+data) z laboratorium 5 (podzbiór z 3 lat) dokonaj analizy ważności cech, ich selekcji, oraz wizualizacji z pomocą redukcji wymiarowości. Zbiór zawiera same zmienne numeryczne.\n",
    "\n",
    "1. Dokonaj analizy korelacji cech. Usuń najmocniej skorelowane cechy, próg dobierz wedle własnego uznania - może się przydać biblioteka [feature-engine](https://feature-engine.trainindata.com/en/1.3.x/user_guide/selection/SmartCorrelatedSelection.html).\n",
    "2. Dokonaj analizy ważności cech z pomocą algorytmu Boruty, realizującego all-relevant feature selection. Przykładowa implementacja jest w bibliotece [ARFS](https://arfs.readthedocs.io/en/latest/Methods%20overview.html). Wykorzystaj klasyczny algorytm Boruty oraz metodę Leshy (z pewnymi modyfikacjami) - [przykład w ARFS](https://arfs.readthedocs.io/en/latest/notebooks/arfs_classification.html). Czy wybrano wszystkie cechy, czy tylko ich podzbiór?\n",
    "3. Porównaj wyniki z mutual information oraz RFECV używającego LightGBM (z domyślnymi hiperparametrami). Czy wyniki różnią się od algorytmu Boruty?\n",
    "4. Wytrenuj LightGBM (pamiętaj o wagach klas) na cechach po selekcji:\n",
    "   - algorytmem Boruty\n",
    "   - algorytmem Leshy\n",
    "   - mutual information\n",
    "   - RFECV\n",
    "5. Dokonaj wizualizacji dla każdego zestawu cech:\n",
    "   - używając liniowej redukcji wymiaru z PCA\n",
    "   - używając nieliniowej redukcji wymiaru z [UMAP](https://umap-learn.readthedocs.io/en/latest/basic_usage.html)\n",
    "   - dla każdego wykresu pokoloruj go prawdziwymi klasami\n",
    "6. Porównaj AUROC na zbiorze testowym oraz wybraną liczbę cech dla każdej z metod. Która metoda twoim zdaniem jest tutaj najlepsza? Która metoda (zestaw cech) daje najlepiej wyglądającą redukcję wymiaru?"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PUM",
   "language": "python",
   "name": "pum"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
