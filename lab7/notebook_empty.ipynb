{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "377e2931-3fd4-48eb-b7c0-84fbadbb3850",
   "metadata": {},
   "source": [
    "# Prognozowanie szeregów czasowych"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba4a24c2-8b01-4e51-81dc-8123b193e3d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "warnings.simplefilter(action=\"ignore\", category=FutureWarning)\n",
    "warnings.simplefilter(action=\"ignore\", category=UserWarning)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "865bafa4-8d2f-4c99-910e-c9f0ea05ac8a",
   "metadata": {},
   "source": [
    "Naszą główną biblioteką do szeregów czasowych będzie [sktime](https://www.sktime.net/en/stable/index.html), oferujące interfejs podobny do Scikit-learn. Jest to wrapper na wiele innych bibliotek, między innymi:\n",
    "- [statsforecast](https://github.com/Nixtla/statsforecast) - wydajna implementacja wielu metod prognozowania, m.in. AutoARIMA i AutoETS\n",
    "- [pmdarima](https://alkaline-ml.com/pmdarima/) - testy statystyczne dla szeregów czasowych oraz alternatywna implementacja AutoARIMA\n",
    "- [statsmodels](https://www.statsmodels.org/stable/index.html) - niektóre metody dekompozycji szeregów czasowych i prognozowania\n",
    "\n",
    "Do obliczania testów statystycznych wykorzystamy [scipy](https://docs.scipy.org/doc/scipy/index.html) oraz [statsmodels](https://www.statsmodels.org/stable/index.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35140fe1-51b5-4486-bd9a-c11e7e6fa570",
   "metadata": {},
   "source": [
    "## Prognozowanie inflacji w Polsce"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e00201d3-569e-478c-87d2-066d76546496",
   "metadata": {},
   "source": [
    "Problem przewidywania inflacji (wskaźników cen towarów i usług konsumpcyjnych) jest bardzo powszechny, i wykonuje go zasadniczo każde państwo i większa instytucja finansowa. W praktyce jest to cała grupa interesujących problemów, bo mamy inflację, inflację bazową (po wyłączeniu najbardziej zmiennych czynników, np. cen żywności), \n",
    "\n",
    "W Polsce podstawowe dane o inflacji [publikuje Główny Urząd Statystyczny (GUS)](https://stat.gov.pl/obszary-tematyczne/ceny-handel/wskazniki-cen/wskazniki-cen-towarow-i-uslug-konsumpcyjnych-pot-inflacja-/miesieczne-wskazniki-cen-towarow-i-uslug-konsumpcyjnych-od-1982-roku/), z częstotliwością miesięczną, kwartalną, półroczną i roczną. Bardziej szczegółowe informacje publikują inne instytucje, bo zależą od przyjętej metodyki, np. inflację bazową [oblicza i publikuje Narodowy Bank Polski (NBP)](https://nbp.pl/statystyka-i-sprawozdawczosc/inflacja-bazowa/).\n",
    "\n",
    "Prognozowanie inflacji jest wyzwaniem, bo typowo inflacja:\n",
    "- ma wyraźne cykle, ale wysoce nieregularne\n",
    "- jest implicite związana z wieloma czynnikami zewnętrznymi (gospodarka światowa, decyzje polityczne etc.)\n",
    "- nie ma wyraźnej sezonowości\n",
    "- interesuje nas prognozowanie na wielu poziomach szczególowości, np. miesięczne, kwartalne, roczne\n",
    "\n",
    "Zajmiemy się danymi z GUS, o częstotliwości miesięcznej. Aby otrzymać inflację rok do roku (RDR), z którą typowo mamy do czynienia, trzeba odjąć 100 od podawanych wartości."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2ffa2d6-a1ec-4c91-8998-a104f91597b9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"polish_inflation.csv\")\n",
    "df = df.rename(columns={\"Rok\": \"year\", \"Miesiąc\": \"month\", \"Wartość\": \"value\"})\n",
    "\n",
    "# create proper date column\n",
    "df[\"day\"] = 1\n",
    "df[\"date\"] = pd.to_datetime(df[[\"year\", \"month\", \"day\"]])\n",
    "df[\"date\"] = df[\"date\"].dt.to_period(\"M\")\n",
    "\n",
    "# set datetime index\n",
    "df = df.set_index(df[\"date\"], drop=True)\n",
    "df = df.sort_index()\n",
    "\n",
    "# leave only time series values\n",
    "df = df[\"value\"] - 100\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5edfd8d-7ed8-4a74-8771-b9c6860a5a85",
   "metadata": {},
   "source": [
    "Do rysowania szeregów czasowych najłatwiej użyć funkcji `plot_series` z sktime, która ustawi nam od razu ładnie opisy na osi X."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be1d766f-c747-440d-80af-ee5a430514ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sktime.utils.plotting import plot_series\n",
    "\n",
    "plot_series(df, title=\"Polish inflation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68125ed4-9c94-4823-a3f1-5474cfd290ca",
   "metadata": {},
   "source": [
    "Tu nie ma błędu - lata 90 były ciekawym okresem, z [hiperinflacją](https://pl.wikipedia.org/wiki/Hiperinflacja#Polska_%E2%80%93_lata_80._XX_wieku), późniejszą [terapią szokową](https://en.wikipedia.org/wiki/Shock_therapy_(economics)) i realizacją [planu Balcerowicza](https://pl.wikipedia.org/wiki/Plan_Balcerowicza). Z perspektywy prognozowania szeregów czasowych jest to ewidentny outlier, ale przy tym bardzo długi. W związku z tym ograniczymy się do późniejszego okresu, od roku 2000."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cf8e794-7430-412c-843c-9c5bdb5fa7e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df.index >= \"2000-01\"]\n",
    "plot_series(df, title=\"Polish inflation, from year 2000\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1e593b4-9e1a-4ce9-916c-caf179b36b07",
   "metadata": {},
   "source": [
    "Już \"na oko\" widać tutaj ewidentne cykle i trendy. Natomiast są też dobre wieści - inflacja jest dość łagodnie zmiennym agregatem. Pytanie, co z sezonowością?\n",
    "\n",
    "**Zadanie 1 (0.5 punktu)**\n",
    "\n",
    "Uzupełnij kod funkcji `plot_stl_decomposition`. Wykorzystaj `STLTransformer` do obliczenia dekompozycji STL ([dokumentacja](https://www.sktime.net/en/v0.29.0/api_reference/auto_generated/sktime.transformations.series.detrend.STLTransformer.html)). Pamiętaj o podaniu odpowiednich opcji, aby uwzględnić podany okres sezonowości, i zwrócić wszystkie trzy komponenty.\n",
    "\n",
    "Następnie narysuj dekompozycję STL dla danych o inflacji. Skomentuj:\n",
    "- czy twoim zdaniem widać tutaj roczną sezonowość?\n",
    "- czy rezydua są białym szumem, czy widać tam jeszcze jakieś informacje do wykorzystania?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a86ff53a-a5bd-4fb3-a02b-e7ff4cd475a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sktime.transformations.series.detrend import STLTransformer\n",
    "\n",
    "\n",
    "def plot_stl_decomposition(data: pd.Series, seasonal_period: int = 12) -> None:\n",
    "    ...\n",
    "\n",
    "    fig, axs = plt.subplots(4, figsize=(15, 10))\n",
    "\n",
    "    # plot series, trend, seasonality and residuals in subplots\n",
    "    ...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d25ff1c8-fd63-4e5a-90f1-5aa7187748e2",
   "metadata": {},
   "source": [
    "// skomentuj tutaj"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c212cf20-957e-4ab7-a3b4-ed7f98babed1",
   "metadata": {},
   "source": [
    "Metoda \"na oko\" z dekompozycją STL jest ważna - to pozwala nabrać intuicji i wiedzy co do danych, i walidować parametry. Ale od tego są procedury automatyczne, z testami statystycznymi, żeby nie trzeba było tak ręcznie.\n",
    "\n",
    "Sprawdźmy teraz, jak wygląda kwestia sezonowości i stacjonarności. Nie jest to stricte potrzebne dla modeli ETS - one przyjmują dane as-is. Natomiast dla modeli ARIMA jest to już niezbędne, bo wymagają danych stacjonarnych, a wiedza o sezonowości potrafi bardzo przyspieszyć obliczenia (SARIMA jest dużo wolniejsza).\n",
    "\n",
    "**Zadanie 2 (0.75 punktu)**\n",
    "\n",
    "1. Sprawdź za pomocą testów statystycznych dla sezonowości, czy w danych występuje sezonowość kwartalna, półroczna lub roczna. Przyda się funkcja `nsdiffs` z biblioteki pmdarima. Jeżeli wykryto sezonowość, to usuń ją z pomocą klasy `Differencer` z sktime ([dokumentacja](https://www.sktime.net/en/stable/api_reference/auto_generated/sktime.transformations.series.difference.Differencer.html)) oraz narysuj wartości na wykresie.\n",
    "\n",
    "2. Sprawdź za pomocą testów statystycznych dla stacjonarności, jaki rząd różnicowania stacjonaryzuje szereg. Przyda się funkcja `ndiffs` z biblioteki pmdarima. Jeżeli jest większy niż zero, to usuń stacjonarość za pomocą klasy `Differencer` i narysuj wykres wartości po różnicowaniu.\n",
    "\n",
    "3. Skomentuj, z jakiego wariantu modeli ARIMA byś skorzystał i dlaczego, na podstawie obecnej wiedzy: ARMA, ARIMA czy SARIMA.\n",
    "\n",
    "Skorzystaj z domyślnych wartości `D_max` oraz `d_max`.\n",
    "\n",
    "**Uwaga:** stwórz tutaj nowe zmienne dla wartości po różnicowaniu, nie nadpisuj `df`. Jeszcze nam się przyda."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a05ff6d6-f238-49de-bd37-19f3490e346b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "dac3c4e1-699f-42c3-aefd-6d1ecabd20d8",
   "metadata": {},
   "source": [
    "// skomentuj tutaj"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76688e73-8a44-4f06-b888-d872f50e809a",
   "metadata": {},
   "source": [
    "Jesteśmy już zasadniczo gotowi do treningu modeli. Do testowania wykorzystamy 20% najnowszych danych, ze strategią expanding window, i krokiem 1 (bo mamy odczyty inflacji co miesiąc). Naszymi metrykami będą MAE, SMAPE oraz MASE.\n",
    "\n",
    "Wykonamy też analizę rezyduów. Błędy powinny mieć rozkład normalny (model bez biasu) oraz nie mieć autokorelacji (model wykorzystujący całą wiedzę). Na potrzeby wszystkich testów statystycznych zakładamy poziom istotności $\\alpha = 0.05$.\n",
    "\n",
    "Wykorzystamy test normalności Anderson-Darling, bo jest nieco \"luźniejszy\" niż test Shapiro-Wilk, co jest przydatne w praktyce - błędy rzadko kiedy są bardzo blisko rozkładu normalnego. Hipotezą zerową jest, że wartości pochodzą z danego rozkładu (domyślnie normalnego), a alternatywną, że z jakiegoś innego.\n",
    "\n",
    "Dla testowania autokorelacji błędów użyjemy testu Ljung-Box, który testuje autokorelację błędów dla różnych lagów. Dla każdego laga ma osobny test, w którym hipotezą zerową jest brak autokorelacji, a hipotezą alternatywą autokorelacja błędów dla danego opóźnienia.\n",
    "\n",
    "**Zadanie 3 (1.5 punktu)**\n",
    "\n",
    "Uzupełnij kod funkcji `evaluate_model`:\n",
    "1. Stwórz `ExpandingWindowSplitter` ([dokumentacja](https://www.sktime.net/en/stable/api_reference/auto_generated/sktime.split.ExpandingWindowSplitter.html)), który zacznie swoje działanie od 80% danych. Wielkość okna wyznacza argument `horizon`.\n",
    "2. Stwórz listę metryk składającą się z MAE, SMAPE oraz MASE, z biblioteki sktime.\n",
    "3. Przeprowadź ewaluację modelu za pomocą funkcji `evaluate` ([dokumentacja](https://www.sktime.net/en/stable/api_reference/auto_generated/sktime.forecasting.model_evaluation.evaluate.html)). Przekaż `return_data=True`, żeby zwrócić też obliczone predykcje. Zwraca ona DataFrame z wynikami.\n",
    "4. Oblicz średnie wartości metryk z wynikowego DataFrame'a. Wypisz je zaokrąglone do 2 miejsc po przecinku.\n",
    "5. Uwzględniając argument `analyze_residuals`, przeprowadź analizę błędów:\n",
    "   - oblicz rezydua jako $y - \\hat{y}$\n",
    "   - narysuj histogram rezyduów\n",
    "   - wykonaj test Anderson-Darling z biblioteki `scipy` i wypisz, czy rozkład jest normalny, czy nie\n",
    "   - przetestuj test Ljung-Box z biblioteki `statsmodels` i wypisz wyniki testu\n",
    "\n",
    "Następnie przetestuj tę funkcję na dwóch baseline'ach: średniej oraz ostatniej znanej wartości. Skorzystaj z klasy `NaiveForecaster` ([dokumentacja](https://www.sktime.net/en/stable/api_reference/auto_generated/sktime.forecasting.naive.NaiveForecaster.html)), i horyzontu czasowego 3 miesięcy. Narysuj też wykresy predykcji."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9cb3c0e-ab5e-4f2a-9a9c-daaff029023f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import anderson\n",
    "from sktime.forecasting.model_evaluation import evaluate\n",
    "from sktime.performance_metrics.forecasting import (\n",
    "    MeanAbsoluteScaledError,\n",
    "    MeanAbsoluteError,\n",
    "    MeanAbsolutePercentageError,\n",
    ")\n",
    "from sktime.forecasting.model_selection import ExpandingWindowSplitter\n",
    "from statsmodels.stats.diagnostic import acorr_ljungbox\n",
    "\n",
    "\n",
    "def evaluate_model(\n",
    "    model,\n",
    "    data: pd.Series,\n",
    "    horizon: int = 1,\n",
    "    plot_forecasts: bool = False,\n",
    "    analyze_residuals: bool = False,\n",
    ") -> None:\n",
    "    cv = ...\n",
    "    metrics = []\n",
    "    results = ...\n",
    "\n",
    "    # extract and print metrics\n",
    "    mae = ...\n",
    "    smape = ...\n",
    "    mase = ...\n",
    "\n",
    "    y_pred = pd.concat(results[\"y_pred\"].values)\n",
    "\n",
    "    if plot_forecasts:\n",
    "        y_true = data[y_pred.index]\n",
    "        plot_series(data, y_pred, labels=[\"y\", \"y_pred\"])\n",
    "        plt.show()\n",
    "        plt.clf()\n",
    "    \n",
    "    if analyze_residuals:\n",
    "        ...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d02d2f2-4af9-4143-8404-78c0812498d0",
   "metadata": {},
   "source": [
    "Mamy już pierwsze baseline'y, wyniki z nich wyglądają dość solidnie. Zobaczmy, czy ETS albo ARIMA będą w stanie je przebić.\n",
    "\n",
    "**Zadanie 4 (0.75 punktu)**\n",
    "\n",
    "1. Dokonaj predykcji algorytmem AutoETS (klasa `StatsForecastAutoETS`), w wariancie damped trend. Narysuj też predykcje i dokonaj analizy błędów.\n",
    "2. Analogicznie, dokonaj predykcji algorytmem AutoARIMA (klasa `StatsForecastAutoARIMA`). Jeżeli nie wykryto wcześniej sezonowości, to przekaż odpowiednią opcję, żeby ją wyłączyć - SARIMA jest dużo wolniejsza niż ARIMA.\n",
    "3. Skomentuj wyniki:\n",
    "   - czy udało się przebić baseline'y?\n",
    "   - który z modeli jest lepszy, i o czym może to świadczyć?\n",
    "   - czy modele są, przynajmniej w przybliżeniu, poprawne, tzn. mają normalne i nieskorelowane błędy?\n",
    "   - czy wyniki najlepszego modelu są subiektywnie zadowalające?\n",
    "\n",
    "Wykonaj tutaj predykcje 3-miesięczne, tak jak wcześniej."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6a17e8d-60f3-4324-aed2-733f8e796219",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "47a0786b-5b08-4a46-b853-8b590b7a2346",
   "metadata": {},
   "source": [
    "// skomentuj tutaj"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb51b89b-1a61-4bc3-9e02-dc14b28ff452",
   "metadata": {},
   "source": [
    "Prognozowanie 3-miesięczne było jednak dość krótkim horyzontem czasowym. Pytanie, co z dalszymi - półrocznym i rocznym. Są one często równie, albo nawet bardziej interesujące, np. w kontekście planowania budżetowego.\n",
    "\n",
    "**Zadanie 5 (0.75 punktu)**\n",
    "\n",
    "Dokonaj prognoz dla 6-miesięcznego oraz rocznego horyzontu:\n",
    "- 2 baseline'ami\n",
    "- metodami ETS oraz ARIMA\n",
    "- dla najlepszego modelu narysuj prognozy oraz dokonaj analizy błędów\n",
    "\n",
    "Skomentuj:\n",
    "- czy widać jakieś zmiany między modelami w stosunku do przypadku 3-miesięcznego?\n",
    "- jak zmienia się jakość prognoz przy większych horyzontach czasowych?\n",
    "- czy te modele dla dłuższych horyzontów są twoim zdaniem użyteczne?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0925cd49-984e-4a13-805d-451789c27389",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "94726e94-4efe-439a-9c8b-07901b2e8c6b",
   "metadata": {},
   "source": [
    "// skomentuj tutaj"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b0ae0c7-35b8-43d2-90f1-a3f99e65eb01",
   "metadata": {},
   "source": [
    "## Prognozowanie ruchu sieciowego"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "858e6b60-5c1d-405b-8ba2-f5bd7dc4783f",
   "metadata": {},
   "source": [
    "Teraz zajmiemy się problemem o zupełnie innej charakterystyce - prognozowaniem ruchu sieciowego. Jest to kluczowe zadanie pod kątem skalowania serwerów, coraz częściej realizowane z pomocą właśnie prognozowania szeregów czasowych. Tzw. predictive scaling implementują między innymi [AWS](https://docs.aws.amazon.com/autoscaling/ec2/userguide/ec2-auto-scaling-predictive-scaling.html), [GCP](https://cloud.google.com/compute/docs/autoscaler/predictive-autoscaling) i [Azure](https://learn.microsoft.com/en-us/azure/azure-monitor/autoscale/autoscale-predictive), istnieją także rozwiązania dla Kubernetesa, [open source](https://predictive-horizontal-pod-autoscaler.readthedocs.io/en/latest/), oraz [komercyjne](https://keda.sh/blog/2022-02-09-predictkube-scaler/). Prognozowanie szeregów czasowych pozwala proaktywnie dokładać kolejne maszyny wirtualne, aby przygotować się na zwiększony ruch i zapewnić niskie opóźnienia, oraz redukuje koszty, automatycznie wyłączając je przy przewidywanym małym ruchu.\n",
    "\n",
    "Wikipedia i Google hostowały [konkurs na platformie Kaggle](https://www.kaggle.com/c/web-traffic-time-series-forecasting), w którym zadaniem było prognozowanie ruchu na poszczególnych stronach. Jest on dość kolosalny, dlatego na jego podstawie przygotowano uproszczony, w którym mamy zapisaną sumaryczną liczbę requestów do Wikipedii, liczoną w milionach.\n",
    "\n",
    "Charakterystyki takiego zadania to typowo:\n",
    "- krótkoterminowe prognozy\n",
    "- duża częstotliwość danych\n",
    "- dynamicznie zmienne i zaszumione dane (np. przez aktywność botów)\n",
    "- bardzo częsty retrening modeli\n",
    "- duża potrzeba automatyzacji, brak ręcznej analizy modeli"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6aef42e-b807-4528-aa29-5b93d695be96",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_parquet(\"wikipedia_traffic.parquet\")\n",
    "df = df.set_index(\"date\").to_period(freq=\"d\")\n",
    "plot_series(df)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35fb9066-ac63-4b38-908b-c2fd712ecd09",
   "metadata": {},
   "source": [
    "**Zadanie 6 (1 punkt)**\n",
    "\n",
    "Zakładając horyzont 1 dnia, wytrenuj modele i dokonaj ich ewaluacji (analogicznie do poprzedniego zbioru danych):\n",
    "- 2 modele baseline'owe\n",
    "- model ETS (z damped trend)\n",
    "- model ARIMA (bez sezonowości)\n",
    "- model SARIMA\n",
    "\n",
    "Skomentuj:\n",
    "- czy z tych wyników można wnioskować, że mamy tu sezonowość?\n",
    "- czy udało się przebić baseline?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5f96ad7-0dfd-4a58-8355-3ed9cd888d66",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5d489afe-bb56-4006-a38a-c14fc983fb61",
   "metadata": {},
   "source": [
    "Może jednak da się lepiej? Nasze dane mają bardzo dużą zmienność, a więc wariancję, a modele ARIMA tego nie lubią. Zastosujmy więc transformacje stabilizujące wariancję. Mamy tu same dodatnie wartości, więc nie będzie tu problemów numerycznych i można używać dowolnych operacji.\n",
    "\n",
    "Trzeba tutaj wykorzystać `Pipeline` z sktime, bo automatycznie odwróci on wszystkie operacje podczas predykcji. Czasem dokonuje się ewaluacji danych po transformacji, ale typowo interesuje nas jakość na pierwotnych danych. Przekształcenia służą tylko do poprawienia treningu.\n",
    "\n",
    "**Zadanie 7 (0.5 punktu)**\n",
    "\n",
    "Stwórz pipeline, składający się z transformacji oraz modelu AutoARIMA (bez sezonowości). Wypróbuj transformacje:\n",
    "- log\n",
    "- sqrt\n",
    "- Box-Cox\n",
    "\n",
    "Przedstaw na wykresie oraz pierwotny szereg czasowy szereg po transformacji dającej najlepszy wynik.\n",
    "\n",
    "Funkcję `make_pipeline` oraz klasy implementujące transformacje znajdziesz w sktime.\n",
    "\n",
    "Skomentuj:\n",
    "- czy udało się poprawić wynik transformacją?\n",
    "- oceniając wizualnie, czy udało się ustabilizować wariancję?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a150d56-dc44-444c-89ed-d7227bbcff91",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "80e27ab2-5043-44fd-b1c3-02df2fc8b50a",
   "metadata": {},
   "source": [
    "## Prognozowanie sprzedaży"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "375400a6-3e06-43c7-8130-92e42d0f1d5f",
   "metadata": {},
   "source": [
    "Najpowszechniejszym zastosowaniem prognozowania szeregów czasowych jest przewidywanie zapotrzebowania, sprzedaży, popytu, wydatków etc., czyli wszystkich typowych wskaźników dla przedsiębiorstwa. Musi to robić zasadniczo każda firma, dlatego nawet Excel oferuje rozbudowane możliwości prognozowania szeregów czasowych.\n",
    "\n",
    "Zajmiemy się teraz kluczowym zadaniem dla włoskiej gospodarki, jakim jest prognozowanie sprzedaży makaronu. Zbiór danych został zebrany przez włoskich naukowców na potrzeby [tego artykułu naukowego](https://www.sciencedirect.com/science/article/abs/pii/S0957417421005431?via%3Dihub). Dane pochodzą z lat 2014-2018 (do końca roku), z 4 firm, i dotyczą wielu wyrobów z makaronu. Zawierają też dane o promocjach na poszczególne produkty. Niektórych danych brakuje, i wartości te trzeba imputować.\n",
    "\n",
    "Dane tego typu mają typowo następujące cechy:\n",
    "- często trend rosnący, mniejszy lub większy\n",
    "- mocną sezonowość, często więcej niż jedną\n",
    "- dużą czułość na powtarzalne okazje, np. weekendy czy święta\n",
    "- duże i powtarzalne outliery\n",
    "- dość duży szum i zmienność\n",
    "- relatywnie niską częstotliwość, dzienną lub niższą\n",
    "- często długie horyzonty dla prognozowania: miesięczne, kwartalne czy roczne\n",
    "- zmienne egzogeniczne"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c35fd51-5c4c-4bb3-8f6f-662260c40537",
   "metadata": {},
   "source": [
    "**Zadanie 8 (1 punkt)**\n",
    "\n",
    "1. Wczytaj dane z pliku `\"italian_pasta.csv\"`\n",
    "2. Wybierz kolumny z firmy B1 (mają `\"B1\"` w nazwie) oraz kolumnę `\"DATE\"`\n",
    "3. Stwórz kolumnę `\"value\"`:\n",
    "   - sumaryczna sprzedaż makaronu\n",
    "   - suma kolumn z `\"QTY\"` w nazwie\n",
    "4. Stwórz kolumnę `\"num_promos\"`:\n",
    "   - sumaryczna liczba aktualnych promocji\n",
    "   - suma kolumn z `\"PROMO\"` w nazwie\n",
    "5. Pozostaw tylko kolumny `\"DATE\"`, `\"value\"` oraz `\"num_promos\"`\n",
    "6. Stwórz indeks typu datetime:\n",
    "   - zmień typ kolumny `\"DATE\"` na datetime\n",
    "   - ustaw tę kolumnę jako indeks\n",
    "   - ustaw jej częstotliwość (frequency) jako dzienną, `\"d\"`\n",
    "7. Podziel dane na:\n",
    "   - zmienną `y`, `pd.Series` z kolumny `\"value\"`, główny szereg czasowy\n",
    "   - zmienną `X`, `pd.Series` z kolumny `\"num_promos\"`, zmienne egzogeniczne\n",
    "8. Uzupełnij wartości brakujące w zmiennych egzogenicznych zerami - możemy założyć, że domyślnie nie ma promocji.\n",
    "9. Przedstaw na wykresie szereg czasowy `y`. Pamiętaj o podaniu tytułu wykresu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b8d1b17-432a-473e-b600-2f0d8dcb3657",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "faf3b554-1f5c-4a68-a4ee-df7dabee6deb",
   "metadata": {},
   "source": [
    "Tym razem będzie nas interesowało prognozowanie w długim horyzoncie czasowym. Zakładamy, że nasz klient, włoski producent makaronu, ma dane z lat 2014-2017, i chce prognozować swoją sprzedaż w roku 2018. Takie prognozy są potrzebne np. do zakontraktowania długoterminowych dostaw i produkcji na kolejny rok. Z naszej perspektywy jest to trudne, ale przynajmniej szybkie, bo mamy tylko jeden zbiór treningowy i testowy, a nie serię, jak w expanding window.\n",
    "\n",
    "Do ewaluacji posłuży nam teraz funkcja `evaluate_pasta_sales_model`.\n",
    "\n",
    "**Zadanie 9 (1 punkt)**\n",
    "\n",
    "Uzupełnij kod funkcji do ewaluacji:\n",
    "- podziel `y` na zbiór treningowy i testowy według daty, wszystko od `2018-01-01` włącznie to zbiór testowy\n",
    "- jeżeli przekazano `X`, to je tak samo podziel\n",
    "- dokonaj imputacji wartości brakujących w `y`:\n",
    "  - klasa `Imputer` z sktime\n",
    "  - zastosuj strategię `ffill` (uzupełnienie ostatnią znaną wartością)\n",
    "- wytrenuj model (pamiętaj o przekazaniu `X`), dokonaj predykcji\n",
    "- dokonaj ewaluacji predykcji za pomocą MAE, SMAPE oraz MASE (znajdź odpowiednie funkcje w sktime)\n",
    "- wypisz wyniki do 2 miejsca po przecinku\n",
    "- skopiuj kod dla `plot_forecasts` i `analyze_residuals` z zadania 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce58f021-c0ba-4d67-a245-b528912b0298",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional\n",
    "\n",
    "import numpy as np\n",
    "from sktime.transformations.series.impute import Imputer\n",
    "\n",
    "\n",
    "def evaluate_pasta_sales_model(\n",
    "    model,\n",
    "    df: pd.Series,\n",
    "    X: Optional[np.ndarray] = None,\n",
    "    plot_forecasts: bool = False,\n",
    "    analyze_residuals: bool = False,\n",
    ") -> None:\n",
    "    # split data\n",
    "\n",
    "    # impute values\n",
    "\n",
    "    # fit, predict, calculate and print metrics\n",
    "\n",
    "    # plot_forecasts\n",
    "\n",
    "    # analyze_residuals\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbb901ea-0be6-4fb0-81af-1f95010eda49",
   "metadata": {},
   "source": [
    "Teraz nie pozostaje nam nic, tylko prognozować.\n",
    "\n",
    "**Zadanie 10 (1.5 punktu)**\n",
    "\n",
    "Dokonaj predykcji modelami:\n",
    "- 2 baseline'ami\n",
    "- ETS (z damped trend)\n",
    "- ARIMA\n",
    "- SARIMA z sezonowością 30 dni\n",
    "- ARIMAX\n",
    "- SARIMAX z sezonowością 30 dni\n",
    "\n",
    "Dla najlepszego modelu wypróbuj transformacje log, sqrt oraz Box-Cox.\n",
    "\n",
    "Na koniec narysuj predykcje oraz dokonaj analizy błędów dla finalnego modelu.\n",
    "\n",
    "Skomentuj:\n",
    "- czy udało się przebić baseline?\n",
    "- czy finalny model uwzględnia sezonowość i/lub zmienne egzogeniczne (dane o promocjach)?\n",
    "- czy warto było stosować transformacje danych?\n",
    "- skomentuj ogólne zachowanie modelu na zbiorze testowym, na podstawie wykresu predykcji\n",
    "- czy model jest nieobciążony (rozkład normalny błędów o średniej 0), bez autokorelacji, czy jest tutaj miejsce na poprawę?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8b87bd6-c137-49d1-a67a-f771ddc5f3ba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ecfa98c8-ce60-4cd8-8547-5649fd7a8578",
   "metadata": {},
   "source": [
    "Zmienne egzogeniczne można jeszcze rozbudować - przykładowo, zachowanie kupujących w weekendy i święta jest inne. Typowo sprzedaż mocno rośnie przed i po okresach, kiedy sklepy są zamknięte, no i naturalnie spada do zera, kiedy sklep jest nieczynny.\n",
    "\n",
    "**Zadanie 11 (0.75 punktu)**\n",
    "\n",
    "1. Stwórz listę zmiennych reprezentujących święta z pomocą klasy `HolidayFeatures`:\n",
    "   - przyda się funkcja `country_holidays` z biblioteki `holidays`\n",
    "   - pamiętaj o tym, że operujemy we Włoszech, skrót kraju `\"IT\"`\n",
    "   - uwzględnij weekendy\n",
    "   - stwórz po prostu jedną zmienną \"czy jest święto\" (opcje `return_dummies` i `return_indicator`)\n",
    "2. Dodaj te zmienne do naszych danych o promocjach `X`. Przyda się `pd.merge` oraz opcje `left_index` i `right_index`.\n",
    "3. Wytrenuj model ARIMAX (lub SARIMAX, jeżeli wcześniej wykryto sezonowość). Wykorzystaj najlepszą transformację z poprzedniego zadania.\n",
    "4. Skomentuj, jak mają się jego wyniki do wcześniejszych."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff34c7b9-1a2b-4545-bff7-d4d7e3c2d975",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "// skomentuj tutaj",
   "id": "63e59f0fda437be9"
  },
  {
   "cell_type": "markdown",
   "id": "42aa693c-129c-4c0f-b2ed-748fcfccaea8",
   "metadata": {},
   "source": [
    "## Zadanie dodatkowe (3 punkty)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28d0fbf1-9d6c-498c-96d0-29f9a65a6139",
   "metadata": {},
   "source": [
    "Klasyfikacja szeregów czasowych często wykorzystuje dane z akcelerometrów, smartfonów, smartwatchy i innych tego typu urządzeń. Praktycznie każdy wyżej klasy smartwatch ma wbudowane proste klasyfikatory analizujące aktywność, tętno, ruch itp. Ciekawym zastosowaniem miary takiej aktywności dobowej jest diagnostyka medyczna, bo niektóre choroby powodują nieoczywiste na pierwszy rzut oka zmniejszenie motoryki człowieka.\n",
    "\n",
    "[Zbiór danych Depresjon](https://datasets.simula.no/depresjon/) dotyczy diagnozy depresji na podstawie aktywności dobowej mierzonej smartwatchem ActiGraph. Zbiór ze względu na wrażliwą naturę problemu jest bardzo mały, 55 pacjentów, ale trzeba sobie z tym radzić, bo w medycynie jest to dość normalne. Pomiary per pacjent są dość długie, około 2 tygodni.\n",
    "\n",
    "1. Wczytaj dane, upewnij się że mają prawidłowe typy\n",
    "2. Uzupełnij wartości brakujące per pacjent za pomocą interpolacji liniowej (klasa `Imputer`)\n",
    "3. Do ewaluacji wykorzystaj podejście z [tego artykułu](https://www.cai.sk/ojs/index.php/cai/article/view/2021_4_850), tzw. nested cross-validation:\n",
    "   - 5-fold CV (stratified) do testowania (outer)\n",
    "   - LOOCV do wyboru hiperparametrów (inner)\n",
    "4. Dokonaj ekstrakcji cech algorytmami:\n",
    "   - [MiniROCKET](https://www.sktime.net/en/latest/api_reference/auto_generated/sktime.transformations.panel.rocket.MiniRocket.html)\n",
    "   - [Shapelet transform](https://www.sktime.net/en/latest/api_reference/auto_generated/sktime.transformations.panel.shapelet_transform.ShapeletTransform.html)\n",
    "   - [TSFRESH](https://www.sktime.net/en/latest/api_reference/auto_generated/sktime.transformations.panel.tsfresh.TSFreshRelevantFeatureExtractor.html) (w wariancie \"efficient\")\n",
    "   - [Catch22](https://www.sktime.net/en/latest/api_reference/auto_generated/sktime.transformations.panel.catch22.Catch22.html)\n",
    "5. Wykorzystaj regresję logistyczną (`LogisticRegressionCV`) jako klasyfikator\n",
    "6. Dokonaj ewaluacji za pomocą metryk:\n",
    "   - balanced accuracy\n",
    "   - F1\n",
    "   - precision\n",
    "   - recall\n",
    "   - specificity\n",
    "   - AUROC\n",
    "   - MCC (Matthews Correlation Coefficient)\n",
    "7. Porównaj wyniki z [artykułem](https://www.cai.sk/ojs/index.php/cai/article/view/2021_4_850) (tabela 4). Skomentuj, czy udało się przebić te wyniki."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7129bd0f-71e9-440c-b33c-73bb157f101f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
